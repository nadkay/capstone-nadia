# -*- coding: utf-8 -*-
"""Hacktive_Nadia Kayla Dhinita.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vunr0_KDc9UL-J47OETvdTBL9UOg8vj9

Data source : https://www.kaggle.com/datasets/anggapurnama/twitter-dataset-ppkm?select=INA_TweetsPPKM_Labeled_Pure.csv
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install Sastrawi

import pandas as pd
import re
import string
import Sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.model_selection import train_test_split
from tqdm import tqdm

folder_path = "/content/drive/MyDrive/Colab Notebooks/IBM/"

df = pd.read_csv(folder_path + "INA_TweetsPPKM_Labeled_Pure.csv", sep="\t")
print(df.shape)
df.head()

#ambil 2 kolom saja "Tweet" & "sentiment"
df = df[["Tweet", "sentiment"]]
df.head()

# casefolding -> mengubah karakter huruf menjadi huruf kecil
df["Tweet"] = df["Tweet"].str.lower()
df.head()

# menghapus link "http" atau "https"
pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
df["Tweet"] = df["Tweet"].apply(lambda x: re.sub(pattern, '', x))
df.head()

# contoh load punctuation
string.punctuation

#fungsi untuk menghapus punctuation
def remove_punc(text):
  for punc in string.punctuation:
    text = text.replace(punc, " ")
  return text

# menjalankan fungsi remove_punc
df["Tweet"] = df["Tweet"].apply(lambda x: remove_punc(x))
df.head()

#membuat fungsi untuk stopword dan slangword
def formaldanstop(t):
  t = t.split()
  for i,x in enumerate(t):
    if x in SlangS.keys():
      t[i] = SlangS[x]
  return ''.join(' '.join(x for x in t if x not in StopS))

#contoh cara preview 50 text teratas dari tabel
df["Tweet"].values[:50]

#drop duplikasi
df = df.drop_duplicates()
print(df.shape)
df.head()

#meninjau kembali 50 teks teratas di tabel
df["Tweet"].values[:50]

#membuat tambahan stopword
additional_stopwords = [
    "pikodata", "inews", "lampungpostid", "lampungpost", "gtvnews","buletininewssiang","koinpelangiqq", "koinpelangi88", "radiopurwokerto",
    "bandarku", "poker", "infoindonesia",
]

#jalankan stopword removal tambahan
df["Tweet"] = df["Tweet"].apply(lambda x: ' '.join([word for word in x.split() if word not in (additional_stopwords)]))
df.head()

# stopword removal dan stemming menggunakan library Sastrawi
stopword = StopWordRemoverFactory().create_stop_word_remover()
stemmer = StemmerFactory().create_stemmer()

df["Tweet"] = df["Tweet"].apply(lambda x: stopword.remove(x))
df["Tweet"] = df["Tweet"].apply(lambda x: stemmer.stem(x))
df.head()

#memunculkan proporsi label
df['sentiment'].value_counts().plot(kind='bar')

#save data bersih
df.to_csv(folder_path + "cleaned_data.csv", index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

# Correct the folder_path to point to the directory
folder_path = "/content/drive/MyDrive/Colab Notebooks/IBM/"

df = pd.read_csv(folder_path + "cleaned_data.csv")
print(df.shape)
df.head()

df["sentiment"].value_counts()

df.isnull().sum()

#undersampling
min_class_size = df["sentiment"].value_counts().min()
df_s1 = df[df.sentiment==1].sample(n=min_class_size, random_state=42)
df_s2 = df[df.sentiment==2].sample(n=min_class_size, random_state=42)
df_s3 = df[df.sentiment==0]
#gabungkan semua kelas
df = pd.concat([df_s1, df_s2, df_s3])
print(df.shape)
df.head()

#split X dan y
X = df['Tweet']
y = df['sentiment']

#split train dan test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#Word Count

"""

from collections import Counter

def count_unique_words(X_train):
  """Counts the total number of unique words in X_train."""
  all_words = []
  for text in X_train:
    words = text.lower().split()
    all_words.extend(words)
  word_counts = Counter(all_words)
  return len(word_counts)


unique_word_count = count_unique_words(X_train)
print("Total unique words in X_train:", unique_word_count)

unique_word_counts = X_train.apply(lambda x: len(set(x.split())))

plt.figure(figsize=(10, 6))
sns.histplot(unique_word_counts, kde=True)
plt.title('Distribution of Unique Word Counts per Row in X_train')
plt.xlabel('Number of Unique Words')
plt.ylabel('Frequency')
plt.show()

from wordcloud import WordCloud

# Assuming X_train is a pandas Series
all_text = ' '.join(X_train)

# Generate the word cloud
wordcloud = WordCloud(width=800,
                      height=400,
                      background_color='white').generate(all_text)

# Display the generated image:
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Word Cloud of X_train')
plt.show()

"""#Feature Extraction"""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    stop_words='english',
    ngram_range=(1,3),
    max_features=50000
)
vectorizer.fit(X_train)
X_train_vectorized = vectorizer.transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

print(X_train_vectorized)

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(X_train_vectorized, y_train)

from sklearn.metrics import accuracy_score, classification_report

y_pred = model.predict(X_test_vectorized)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative (0)', 'Positive (1)', 'Neutral (2)'], yticklabels=['Negative (0)', 'Positive (1)', 'Neutral (2)'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Decision Tree Classifier')
plt.show()

from sklearn.svm import SVC

model = SVC()
model.fit(X_train_vectorized, y_train)

y_pred = model.predict(X_test_vectorized)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative (0)', 'Positive (1)', 'Neutral (2)'], yticklabels=['Negative (0)', 'Positive (1)', 'Neutral (2)'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for SVM')
plt.show()